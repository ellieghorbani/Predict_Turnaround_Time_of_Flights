{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, time \n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_seen_AA_airlines.csv\")\n",
    "print('shape of data:', data.shape)\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHeck our the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def RandomForestClassifier_model(features_data, target_data, rand_state):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_data, target_data, \n",
    "                                                        random_state=rand_state, test_size=0.2)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators= 200, max_depth = 30, random_state=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_test_predict = model.predict(X_test)\n",
    "    #-------------------------------------------------------\n",
    "    cm1 = confusion_matrix(y_test, y_test_predict)\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    \n",
    "    ax.imshow(cm1)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    plt.sca(ax)\n",
    "    #plt.title('Predict Test Data')\n",
    "        \n",
    "    print(cm1.sum(axis=0)[0])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, ('% {}').format(int(cm1[i, j]/cm1.sum(axis= 1)[i]*100)), ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    plt.savefig('confusion_matrix_AA_RandomForestClassifier.png')\n",
    "    print(\"Precision_0 ={:.2f}\".format((cm1[0,0]/(cm1[0,0] + cm1[1,0]))))\n",
    "    print(\"Recall_0 =%f\" %(cm1[0,0]/(cm1[0,0]+cm1[0,1])))\n",
    "    print(\"Precision_1 ={:.2f}\".format((cm1[1,1]/(cm1[1,1] + cm1[0,1]))))\n",
    "    print(\"Recall_1 =%f\" %(cm1[1,1]/(cm1[1,1]+cm1[1,0])))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def gbr_model(features_data, target_data, num_sample, rand_state, name_target):\n",
    "    features_data_sub = features_data.sample(num_sample)\n",
    "    target_data_sub = target_data[features_data_sub.index]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_data_sub, target_data_sub,\n",
    "                                                    random_state=rand_state, test_size=0.2)\n",
    "    gbr = GradientBoostingRegressor(n_estimators = 500, max_depth = 10, min_samples_split = 5,\n",
    "          learning_rate = 0.1, loss= 'ls')\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_test_predict = gbr.predict(X_test)\n",
    "    y_predict = gbr.predict(features_data)\n",
    "    data_frame = pd.DataFrame([[name_target, 'Gradient Boosting Regressor', np.sqrt(sum((y_test - y_test_predict)**2)/len(y_test)),\n",
    "                              np.sqrt(sum((y_test - np.mean(y_test))**2)/len(y_test)),\n",
    "                              np.mean(target_data),np.mean(target_data_sub), np.mean(y_train), \n",
    "                              np.mean(y_test), np.mean(y_test_predict), np.mean(y_predict)]], \n",
    "                              columns = ['name_target','model','RMS_test_predict','RMS_test_mean',\n",
    "                                        'mean_target','mean_sample', 'mean_train', \n",
    "                                        'mean_test', 'mean_test_predict', 'meant_target_predict'])\n",
    "    # Plot the feature importance\n",
    "    \n",
    "    all_features = gbr.feature_importances_\n",
    "    feat_scores_gbr = pd.DataFrame({'Fraction of Samples Affected {} {}'.format('Gradient Boosting', name_target) : all_features}, index=X_train.columns)\n",
    "    feat_scores_gbr = feat_scores_gbr.sort_values(by='Fraction of Samples Affected {} {}'.format('Gradient Boosting', name_target) )\n",
    "    feat_scores_gbr = feat_scores_gbr[-15:]\n",
    "    plt.figure(figsize=(10, 50))\n",
    "    feat_scores_gbr.plot(kind='barh')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    return y_test, y_test_predict,y_predict, data_frame, feat_scores_gbr, gbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_unseen_predict_forest = model_RandomForestClassifier.predict(features_unseen)\n",
    "target_unseen_p = target_unseen.copy()\n",
    "target_unseen_p[target_unseen <= 4] = 1\n",
    "target_unseen_p[target_unseen > 4] = 2\n",
    "### plot category of distrbutions based on Random Forest \n",
    "cm = confusion_matrix(target_unseen_p, target_unseen_predict_forest)\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.imshow(cm)\n",
    "ax.grid(False)\n",
    "ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "ax.set_ylim(1.5, -0.5)\n",
    "ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "plt.sca(ax)\n",
    "plt.title('Predict Unseen Data')\n",
    "    \n",
    "print(cm.sum(axis=0)[0])\n",
    "for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, ('% {}').format(int(cm[i, j]/cm.sum(axis= 1)[i]*100)), ha='center', va='center', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seprate unseen data two distributions without random forest model\n",
    "index1 = target_unseen_p[target_unseen_p == 1].index\n",
    "index2 = target_unseen_p[target_unseen_p == 2].index\n",
    "print( set(index1).issubset(features_unseen.index))\n",
    "features_unseen_1 = features_unseen.loc[index1]\n",
    "features_unseen_2 = features_unseen.loc[index2]\n",
    "target_unseen_1 = target_unseen[index1]\n",
    "target_unseen_2 = target_unseen[index2]\n",
    "print('mean(target_unseen_1)', np.mean(target_unseen_1), 'mean(target_unseen_2)', np.mean(target_unseen_2))\n",
    "#----------------------------------------------------------------\n",
    "#predict distribution of data by useing random forest model\n",
    "index1 = target_unseen_p[target_unseen_predict_forest == 1].index\n",
    "index2 = target_unseen_p[target_unseen_predict_forest == 2].index\n",
    "features_unseen_predict_forest_1 = features_unseen.loc[index1]\n",
    "features_unseen_predict_forest_2 = features_unseen.loc[index2]\n",
    "target_unseen_predict_forest_1 = target_unseen[index1]\n",
    "target_unseen_predict_forest_2 = target_unseen[index2]\n",
    "print('mean(target_unseen_predict_forest_1)', np.mean(target_unseen_predict_forest_1), 'mean(target_unseen_predict_forest_2)', np.mean(target_unseen_predict_forest_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_unseen_predict_forest_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make model to predict TAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1, target_2 = target_seen[target_seen<= 4], target_seen[target_seen> 4]\n",
    "index_1 = target_1.index\n",
    "index_2 = target_2.index\n",
    "features_1, features_2 = features.loc[index_1], features.loc[index_2]\n",
    "print(features_1.shape)\n",
    "print(features_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1_test, target_1_test_predict, target_1_predict, table_gbr_1, feat_scores_gbr_1, model_1 = gbr_model(features_1,target_1, 100000, 1112, 'target_1')\n",
    "#reg_plot(target_1,target_1_test, target_1_test_predict, target_1_predict,'target_1')\n",
    "\n",
    "print('*')\n",
    "target_2_test, target_2_test_predict, target_2_predict, table_gbr_2, feat_scores_gbr_2, model_2 = gbr_model(features_2,target_2, 100000, 1112, 'target_2')\n",
    "#reg_plot(target_2,target_2_test, target_2_test_predict, target_2_predict,'target_2')\n",
    "\n",
    "pd.concat([table_gbr_1, table_gbr_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_models_with_unseen_data(model, name_target, y, x):\n",
    "    \n",
    "    y_predict = model.predict(x)\n",
    "    data_frame = pd.DataFrame([[name_target, 'Gradient Boosting Regressor', np.sqrt(sum((y - y_predict)**2)/len(y)),\n",
    "                              np.sqrt(sum((y - np.mean(y))**2)/len(y)),\n",
    "                              np.mean(y), np.mean(y_predict)]], \n",
    "                              columns = ['name_target','model','RMS_predict','RMS_target_mean',\n",
    "                                        'mean_target','mean_target_predict'])\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_1 = check_models_with_unseen_data(model_1, \"unseen_1\", target_unseen_1, features_unseen_1)\n",
    "table_1_p = check_models_with_unseen_data(model_1, \"unseen_forest_predicted_1\", target_unseen_predict_forest_1, features_unseen_predict_forest_1)\n",
    "table_2 = check_models_with_unseen_data(model_2, \"unseen_2\", target_unseen_2, features_unseen_2)\n",
    "table_2_p = check_models_with_unseen_data(model_2, \"unseen_forest_predicted_2\", target_unseen_predict_forest_2, features_unseen_predict_forest_2)\n",
    "\n",
    "pd.concat([table_1, table_1_p, table_2, table_2_p], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.15/squrt(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
